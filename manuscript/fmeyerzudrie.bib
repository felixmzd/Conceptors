% This file was created with JabRef 2.10b2.
% Encoding: UTF8


@Article{Blake2002,
  Title                    = {Visual competition},
  Author                   = {Blake, Randolph and Logothetis, Nikos K},
  Journal                  = {Nature Reviews Neuroscience},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {13--21},
  Volume                   = {3},

  Owner                    = {Felix},
  Publisher                = {Nature Publishing Group},
  Timestamp                = {2016.03.14}
}

@Article{Brascamp2015,
  Title                    = {The 'laws' of binocular rivalry: 50 years of Levelt's propositions},
  Author                   = {Brascamp, JW and Klink, PC and Levelt, Willem JM},
  Journal                  = {Vision research},
  Year                     = {2015},
  Pages                    = {20--37},
  Volume                   = {109},

  Owner                    = {Felix},
  Publisher                = {Elsevier},
  Timestamp                = {2016.03.14}
}

@Article{Clark2013a,
  Title                    = {Whatever next? Predictive brains, situated agents, and the future of cognitive science},
  Author                   = {Clark, Andy},
  Journal                  = {Behavioral and Brain Sciences},
  Year                     = {2013},
  Number                   = {03},
  Pages                    = {181--204},
  Volume                   = {36},

  Owner                    = {Felix},
  Publisher                = {Cambridge Univ Press},
  Timestamp                = {2016.03.14}
}

@Article{Friston2013,
  Title                    = {Life as we know it},
  Author                   = {Friston, Karl},
  Journal                  = {Journal of The Royal Society Interface},
  Year                     = {2013},
  Number                   = {86},
  Pages                    = {20130475},
  Volume                   = {10},

  Owner                    = {Felix},
  Publisher                = {The Royal Society},
  Timestamp                = {2015.01.15}
}

@Article{Friston2010,
  Title                    = {The free-energy principle: a unified brain theory?},
  Author                   = {Friston, Karl},
  Journal                  = {Nature Reviews Neuroscience},
  Year                     = {2010},
  Number                   = {2},
  Pages                    = {127--138},
  Volume                   = {11},

  Owner                    = {Felix},
  Publisher                = {Nature Publishing Group},
  Timestamp                = {2016.03.14}
}

@Book{Gaehde2014,
  Title                    = {Models, Simulations, and the Reduction of Complexity},
  Author                   = {G{\"a}hde, Ulrich and Hartmann, Stephan and Wolf, J{\"o}rn Henning},
  Publisher                = {Walter de Gruyter},
  Year                     = {2014},
  Volume                   = {4},

  Owner                    = {Felix},
  Timestamp                = {2016.03.14}
}

@Article{Graves2009,
  Title                    = {A novel connectionist system for unconstrained handwriting recognition},
  Author                   = {Graves, Alex and Liwicki, Marcus and Fern{\'a}ndez, Santiago and Bertolami, Roman and Bunke, Horst and Schmidhuber, J{\"u}rgen},
  Journal                  = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  Year                     = {2009},
  Number                   = {5},
  Pages                    = {855--868},
  Volume                   = {31},

  Owner                    = {Felix},
  Publisher                = {IEEE},
  Timestamp                = {2016.03.11}
}

@Book{Helmholtz1925,
  Title                    = {Treatise on Physiological Optics: Translated from the 3rd German Ed},
  Author                   = {von Helmholtz, Hermann and Southall, James Powell Cocke},
  Publisher                = {Optical Society of America},
  Year                     = {1925},

  Owner                    = {Felix},
  Timestamp                = {2016.03.14}
}

@Article{Hochreiter1998,
  Title                    = {The vanishing gradient problem during learning recurrent neural nets and problem solutions},
  Author                   = {Hochreiter, Sepp},
  Journal                  = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  Year                     = {1998},
  Number                   = {02},
  Pages                    = {107--116},
  Volume                   = {6},

  Owner                    = {Felix},
  Publisher                = {World Scientific},
  Timestamp                = {2016.03.11}
}

@Article{Hochreiter1997,
  Title                    = {Long short-term memory.},
  Author                   = {Hochreiter, S. and Schmidhuber, J.},
  Journal                  = {Neural Comput},
  Year                     = {1997},

  Month                    = {Nov},
  Number                   = {8},
  Pages                    = {1735--1780},
  Volume                   = {9},

  Abstract                 = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  Institution              = {Fakult채t f체r Informatik, Technische Universit채t M체nchen, Germany.},
  Keywords                 = {Algorithms; Learning; Memory; Memory, Short-Term; Models, Neurological; Models, Psychological; Nerve Net, physiology; Neural Networks (Computer); Time Factors},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {felix},
  Pmid                     = {9377276},
  Timestamp                = {2016.03.11}
}

@Article{Hohwy2014,
  Title                    = {The Self-Evidencing Brain},
  Author                   = {Hohwy, Jakob},
  Journal                  = {Nouvelle Presse Medicales},
  Year                     = {2014},

  Owner                    = {Felix},
  Publisher                = {Wiley Online Library},
  Timestamp                = {2016.03.14}
}

@Book{Hohwy2013,
  Title                    = {The predictive mind},
  Author                   = {Hohwy, Jakob},
  Publisher                = {OUP Oxford},
  Year                     = {2013},

  Owner                    = {Felix},
  Timestamp                = {2016.03.14}
}

@Article{Hohwy2008,
  Title                    = {Predictive coding explains binocular rivalry: An epistemological review},
  Author                   = {Hohwy, Jakob and Roepstorff, Andreas and Friston, Karl},
  Journal                  = {Cognition},
  Year                     = {2008},
  Number                   = {3},
  Pages                    = {687--701},
  Volume                   = {108},

  Owner                    = {Felix},
  Publisher                = {Elsevier},
  Timestamp                = {2016.03.14}
}

@Article{Ilies2007,
  Title                    = {Stepping forward through echoes of the past: forecasting with echo state networks},
  Author                   = {Ilies, Iulian and Jaeger, Herbert and Kosuchinas, Olegas and Rincon, Monserrat and Sakenas, V and Vaskevicius, N},
  Journal                  = {Short report on the winning entry to the NN3 financial forecasting competition, available online at http://www. neural-forecasting-competition. com/downloads/NN3/methods/27-NN3 Herbert Jaeger report. pdf},
  Year                     = {2007},

  Owner                    = {csguestt},
  Publisher                = {Citeseer},
  Timestamp                = {2014.10.28}
}

@Article{Jaeger2014,
  Title                    = {Controlling recurrent neural networks by conceptors},
  Author                   = {Jaeger, Herbert},
  Journal                  = {arXiv preprint arXiv:1403.3369},
  Year                     = {2014},

  Owner                    = {Felix},
  Timestamp                = {2016.03.07}
}

@Article{Jaeger2001,
  Title                    = {The "echo state" approach to analysing and training recurrent neural networks-with an erratum note},
  Author                   = {Jaeger, Herbert},
  Journal                  = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
  Year                     = {2001},
  Pages                    = {34},
  Volume                   = {148},

  Owner                    = {csguestt},
  Timestamp                = {2014.10.28}
}

@Article{Jaeger2004,
  Title                    = {Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication.},
  Author                   = {Jaeger, Herbert and Haas, Harald},
  Journal                  = {Science},
  Year                     = {2004},

  Month                    = {Apr},
  Number                   = {5667},
  Pages                    = {78--80},
  Volume                   = {304},

  Abstract                 = {We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.},
  Doi                      = {10.1126/science.1091277},
  Institution              = {International University Bremen, Bremen D-28759, Germany. h.jaeger@iu-bremen.de},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {csguestt},
  Pii                      = {304/5667/78},
  Pmid                     = {15064413},
  Timestamp                = {2014.10.09},
  Url                      = {http://dx.doi.org/10.1126/science.1091277}
}

@Book{James2013,
  Title                    = {The principles of psychology},
  Author                   = {James, William},
  Publisher                = {Read Books Ltd},
  Year                     = {2013},

  Owner                    = {Felix},
  Timestamp                = {2016.03.14}
}

@Book{Kloeden2011,
  Title                    = {Nonautonomous dynamical systems},
  Author                   = {Kloeden, Peter E and Rasmussen, Martin},
  Publisher                = {American Mathematical Soc.},
  Year                     = {2011},
  Number                   = {176},

  Owner                    = {Felix},
  Timestamp                = {2016.03.11}
}

@Article{Larger2012,
  Title                    = {Photonic information processing beyond Turing: an optoelectronic implementation of reservoir computing},
  Author                   = {Larger, Laurent and Soriano, Miguel C and Brunner, Daniel and Appeltant, Lennert and Guti{\'e}rrez, Jose M and Pesquera, Luis and Mirasso, Claudio R and Fischer, Ingo},
  Journal                  = {Optics express},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {3241--3249},
  Volume                   = {20},

  Owner                    = {Felix},
  Publisher                = {Optical Society of America},
  Timestamp                = {2016.03.11}
}

@PhdThesis{Levelt1965,
  Title                    = {On binocular rivalry},
  Author                   = {Levelt, Willem JM},
  School                   = {Van Gorcum Assen},
  Year                     = {1965},

  Owner                    = {Felix},
  Timestamp                = {2016.03.14}
}

@Article{Maass2002,
  Title                    = {Real-time computing without stable states: a new framework for neural computation based on perturbations.},
  Author                   = {Maass, Wolfgang and Natschl{\"{a}}ger, Thomas and Markram, Henry},
  Journal                  = {Neural Comput},
  Year                     = {2002},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {2531--2560},
  Volume                   = {14},

  Abstract                 = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
  Doi                      = {10.1162/089976602760407955},
  Institution              = {Institute for Theoretical Computer Science, Technische Universit채t Graz, A-8010 Graz, Austria. maass@igi.tu-graz.ac.at},
  Keywords                 = {Action Potentials, physiology; Computer Simulation; Computer Systems; Computers; Models, Neurological; Neural Networks (Computer); Neurons, physiology},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {csguestt},
  Pmid                     = {12433288},
  Timestamp                = {2014.10.09},
  Url                      = {http://dx.doi.org/10.1162/089976602760407955}
}

@Article{Manjunath2013,
  Title                    = {Echo state property linked to an input: Exploring a fundamental characteristic of recurrent neural networks},
  Author                   = {Manjunath, G and Jaeger, Herbert},
  Journal                  = {Neural computation},
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {671--696},
  Volume                   = {25},

  Owner                    = {Felix},
  Publisher                = {MIT Press},
  Timestamp                = {2016.03.11}
}

@Article{Mozer1989,
  Title                    = {A focused back-propagation algorithm for temporal pattern recognition},
  Author                   = {Mozer, Michael C},
  Journal                  = {Complex systems},
  Year                     = {1989},
  Number                   = {4},
  Pages                    = {349--381},
  Volume                   = {3},

  Owner                    = {Felix},
  Timestamp                = {2016.03.11}
}

@Article{Noest2007,
  Title                    = {Percept-choice sequences driven by interrupted ambiguous stimuli: a low-level neural model},
  Author                   = {Noest, AJ and Van Ee, R and Nijs, MM and Van Wezel, RJA},
  Journal                  = {Journal of vision},
  Year                     = {2007},
  Number                   = {8},
  Pages                    = {10--10},
  Volume                   = {7},

  Owner                    = {Felix},
  Publisher                = {The Association for Research in Vision and Ophthalmology},
  Timestamp                = {2016.03.15}
}

@Article{Paquot2012,
  Title                    = {Optoelectronic reservoir computing.},
  Author                   = {Paquot, Y. and Duport, F. and Smerieri, A. and Dambre, J. and Schrauwen, B. and Haelterman, M. and Massar, S.},
  Journal                  = {Sci Rep},
  Year                     = {2012},
  Pages                    = {287},
  Volume                   = {2},

  Abstract                 = {Reservoir computing is a recently introduced, highly efficient bio-inspired approach for processing time dependent data. The basic scheme of reservoir computing consists of a non linear recurrent dynamical system coupled to a single input layer and a single output layer. Within these constraints many implementations are possible. Here we report an optoelectronic implementation of reservoir computing based on a recently proposed architecture consisting of a single non linear node and a delay line. Our implementation is sufficiently fast for real time information processing. We illustrate its performance on tasks of practical importance such as nonlinear channel equalization and speech recognition, and obtain results comparable to state of the art digital implementations.},
  Doi                      = {10.1038/srep00287},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {csguestt},
  Pmid                     = {22371825},
  Timestamp                = {2014.10.27},
  Url                      = {http://dx.doi.org/10.1038/srep00287}
}

@Book{Pearl2014,
  Title                    = {Probabilistic reasoning in intelligent systems: networks of plausible inference},
  Author                   = {Pearl, Judea},
  Publisher                = {Morgan Kaufmann},
  Year                     = {2014},

  Owner                    = {Felix},
  Timestamp                = {2016.02.27}
}

@Book{Robinson1987,
  Title                    = {The utility driven dynamic error propagation network},
  Author                   = {Robinson, AJ and Fallside, Frank},
  Publisher                = {University of Cambridge Department of Engineering},
  Year                     = {1987},

  Owner                    = {Felix},
  Timestamp                = {2016.03.11}
}

@InCollection{Robinson1996,
  Title                    = {The use of recurrent neural networks in continuous speech recognition},
  Author                   = {Robinson, Tony and Hochberg, Mike and Renals, Steve},
  Booktitle                = {Automatic speech and speaker recognition},
  Publisher                = {Springer},
  Year                     = {1996},
  Pages                    = {233--258},

  Owner                    = {Felix},
  Timestamp                = {2016.03.11}
}

@Book{Sherrington1916,
  Title                    = {The integrative action of the nervous system},
  Author                   = {Sherrington, Charles Scott},
  Publisher                = {CUP Archive},
  Year                     = {1916},

  Owner                    = {Felix},
  Timestamp                = {2016.03.14}
}

@Article{Sutton1986,
  Title                    = {Two problems with backpropagation and oher steepest-descent learning procedures for networks},
  Author                   = {Sutton, Richard S},
  Year                     = {1986},
  Pages                    = {823--831},

  Booktitle                = {Proc. 8th annual conf. cognitive science society}
}

@Article{Tong2006,
  Title                    = {Neural bases of binocular rivalry},
  Author                   = {Tong, Frank and Meng, Ming and Blake, Randolph},
  Journal                  = {Trends in cognitive sciences},
  Year                     = {2006},
  Number                   = {11},
  Pages                    = {502--511},
  Volume                   = {10},

  Owner                    = {Felix},
  Publisher                = {Elsevier},
  Timestamp                = {2016.03.14}
}

@Book{Wade2000,
  Title                    = {A natural history of vision},
  Author                   = {Wade, Nicholas J and Wade, Nicholas},
  Publisher                = {MIT Press},
  Year                     = {2000},

  Owner                    = {Felix},
  Timestamp                = {2016.03.14}
}

@Article{Werbos1988,
  Title                    = {Generalization of backpropagation with application to a recurrent gas market model},
  Author                   = {Werbos, Paul J},
  Journal                  = {Neural Networks},
  Year                     = {1988},
  Number                   = {4},
  Pages                    = {339--356},
  Volume                   = {1},

  Owner                    = {Felix},
  Publisher                = {Elsevier},
  Timestamp                = {2016.03.11}
}

@Article{Wheatstone1838,
  Title                    = {Contributions to the Physiology of Vision.--Part the First. On Some Remarkable, and Hitherto Unobserved, Phenomena of Binocular Vision},
  Author                   = {Wheatstone, Charles},
  Journal                  = {Philosophical transactions of the Royal Society of London},
  Year                     = {1838},
  Pages                    = {371--394},
  Volume                   = {128},

  Owner                    = {Felix},
  Publisher                = {JSTOR},
  Timestamp                = {2016.03.14}
}

@Article{Williams1989,
  Title                    = {A learning algorithm for continually running fully recurrent neural networks},
  Author                   = {Williams, Ronald J and Zipser, David},
  Journal                  = {Neural computation},
  Year                     = {1989},
  Number                   = {2},
  Pages                    = {270--280},
  Volume                   = {1},

  Publisher                = {MIT Press}
}

@Article{Yildiz2012,
  Title                    = {Re-visiting the echo state property},
  Author                   = {Yildiz, Izzet B and Jaeger, Herbert and Kiebel, Stefan J},
  Journal                  = {Neural networks},
  Year                     = {2012},
  Pages                    = {1--9},
  Volume                   = {35},

  Owner                    = {Felix},
  Publisher                = {Elsevier},
  Timestamp                = {2016.03.11}
}

@Article{Zhou2004,
  Title                    = {Perceptual dominance time distributions in multistable visual perception.},
  Author                   = {Zhou, Y. H. and Gao, J. B. and White, K. D. and Merk, I. and Yao, K.},
  Journal                  = {Biol Cybern},
  Year                     = {2004},

  Month                    = {Apr},
  Number                   = {4},
  Pages                    = {256--263},
  Volume                   = {90},

  __markedentry            = {[felix:6]},
  Abstract                 = {Perceptual multistability, alternative perceptions of an unchanging stimulus, gives important clues to neural dynamics. The present study examined 56 perceptual dominance time series for a Necker cube stimulus, for ambiguous motion, and for binocular rivalry. We made histograms of the perceptual dominance times, based on from 307 to 2478 responses per time series (median=612), and compared these histograms to gamma, lognormal and Weibull fitted distributions using the Kolmogorov-Smirnov goodness-of-fit test. In 40 of the 56 tested cases a lognormal distribution provided an acceptable fit to the histogram (in 24 cases it was the only fit). In 16 cases a gamma distribution, and in 11 cases a Weibull distribution, were acceptable but never as the only fit in either case. Any of the three distributions were acceptable in three cases and none provided acceptable fits in 12 cases. Considering only the 16 cases in which a lognormal distribution was rejected ( p<0.05) revealed that minor adjustments to the fourth-moment term of the lognormal characteristic function restored good fits. These findings suggest that random fractal theory might provide insight into the underlying mechanisms of multistable perceptions.},
  Doi                      = {10.1007/s00422-004-0472-8},
  Institution              = {Department of Electrical and Computer Engineering, EB 559, University of Florida, Gainesville, FL 32611, USA.},
  Keywords                 = {Algorithms; Evoked Potentials, physiology; Humans; Models, Neurological; Photic Stimulation, methods; Probability Theory; Psychophysics; Time Factors; Vision, Binocular, physiology; Visual Fields; Visual Perception, physiology},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {felix},
  Pmid                     = {15085344},
  Timestamp                = {2016.06.30},
  Url                      = {http://dx.doi.org/10.1007/s00422-004-0472-8}
}

